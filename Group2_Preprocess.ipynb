{"cells":[{"cell_type":"markdown","source":["# **DATA UNDERSTANDING and PREPROCESSING** <br>\n","\n","In the preprocessing phase of our notebook, we undertook a comprehensive set of tasks to prepare our text data for classification. The following key steps were implemented:\n","\n","1. Text Cleaning\n","2. Feature Enrichment\n","3. Feature Selection\n","4. Imbalanced Learning (Random Undersampling)\n","5. Tokenization and Counting"],"metadata":{"id":"LqbGt35ffW3d"}},{"cell_type":"markdown","source":["#Importing libraries"],"metadata":{"id":"OQ498FcEfYse"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"36P3gtxjfVC5"},"outputs":[],"source":["import pandas as pd\n","import re\n","import numpy as np\n","from nltk.corpus import stopwords\n","import collections as ct\n","from sklearn.feature_extraction.text import CountVectorizer\n","from senticnet.senticnet import SenticNet\n","from sklearn.preprocessing import LabelEncoder\n","sn = SenticNet()\n","le = LabelEncoder()\n","\n","from nltk.corpus import wordnet\n","from nltk.corpus import stopwords\n","from wordfreq import word_frequency\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectFromModel\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.preprocessing import MinMaxScaler\n","import warnings\n","warnings.simplefilter('ignore')"]},{"cell_type":"markdown","metadata":{"id":"9zaQM5ZzfVDJ"},"source":["# First approach to the datasets"]},{"cell_type":"markdown","metadata":{"id":"KwgmTplAfVDN"},"source":["open id - text dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qY66LdKdfVDP","outputId":"3f8c21ef-3f5a-4e18-c343-e77b9127a4dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7775 entries, 0 to 7774\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   id      7775 non-null   int64 \n"," 1   text    7775 non-null   object\n","dtypes: int64(1), object(1)\n","memory usage: 121.6+ KB\n"]}],"source":["alltweet = pd.read_table('dataset_raw/figurative_clean.tsv', header= None)\n","alltweet.columns = ('id', 'text')\n","alltweet.info()"]},{"cell_type":"markdown","metadata":{"id":"LzsCj0IpfVDX"},"source":["open id - score dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YG_DwrkffVDY","outputId":"3e378610-6581-4593-cf88-6db3dbe2da40"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 8000 entries, 0 to 7999\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype\n","---  ------  --------------  -----\n"," 0   id      8000 non-null   int64\n"," 1   score   8000 non-null   int64\n","dtypes: int64(2)\n","memory usage: 125.1 KB\n"]}],"source":["id_tweet = pd.read_excel('dataset_raw/task-11-training-data-integer.xls', header= None)\n","id_tweet.columns = ('id', 'score')\n","id_tweet.info()"]},{"cell_type":"markdown","metadata":{"id":"QoW8oO2ifVDZ"},"source":["check if there're duplicate ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3ZcLdZufVDZ","outputId":"e1ce5b65-4fa7-430b-9134-c40a3d7caa12"},"outputs":[{"data":{"text/plain":["array([], dtype=int64)"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["duplicate_t = np.unique(id_tweet['id'], return_counts= True)\n","indices = np.where(duplicate_t[1] > 1)[0]\n","indices"]},{"cell_type":"markdown","metadata":{"id":"LiDcxriVfVDe"},"source":["open id - frames dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDeii5XHfVDf","outputId":"5c9821bf-e3bc-4bfe-9d2d-a7a3ecbcd112"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7825 entries, 0 to 7824\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   id      7825 non-null   int64 \n"," 1   frame   7225 non-null   object\n","dtypes: int64(1), object(1)\n","memory usage: 122.4+ KB\n"]}],"source":["text_tweet = pd.read_table('dataset_raw/figurative_cleanONLYFRAMES.tsv', header= None)\n","text_tweet.columns = ('id', 'frame')\n","text_tweet.info()"]},{"cell_type":"markdown","metadata":{"id":"Ws8h2I73fVDh"},"source":["merge of the datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1bX6YILfVDh","outputId":"d0d2870b-b9ac-455d-e6e0-9f8506b03cd8"},"outputs":[{"name":"stdout","output_type":"stream","text":["(8336, 4)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>score</th>\n","      <th>text</th>\n","      <th>frame</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>472189928340606976</td>\n","      <td>-4</td>\n","      <td>I just love working for 6.5 hours without a br...</td>\n","      <td>Ranked_expectation Experiencer_focus Measure_d...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>472440774785650688</td>\n","      <td>-4</td>\n","      <td>The happy song does not invoke good feelings. ...</td>\n","      <td>Text Desirability Expertise Social_interaction...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>473085653454827520</td>\n","      <td>-2</td>\n","      <td>Having to run to the train first thing in the ...</td>\n","      <td>Cause_impact Cause_motion Impact Self_motion V...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>463445012374499328</td>\n","      <td>-1</td>\n","      <td>@OmniJerBear haha should have had  at the end</td>\n","      <td>Process_end</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>463501257110724610</td>\n","      <td>-1</td>\n","      <td>Really excited for these last few days of scho...</td>\n","      <td>Calendric_unit Measure_duration Timespan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   id  score  \\\n","0  472189928340606976     -4   \n","1  472440774785650688     -4   \n","2  473085653454827520     -2   \n","3  463445012374499328     -1   \n","4  463501257110724610     -1   \n","\n","                                                text  \\\n","0  I just love working for 6.5 hours without a br...   \n","1  The happy song does not invoke good feelings. ...   \n","2  Having to run to the train first thing in the ...   \n","3      @OmniJerBear haha should have had  at the end   \n","4  Really excited for these last few days of scho...   \n","\n","                                               frame  \n","0  Ranked_expectation Experiencer_focus Measure_d...  \n","1  Text Desirability Expertise Social_interaction...  \n","2  Cause_impact Cause_motion Impact Self_motion V...  \n","3                                        Process_end  \n","4           Calendric_unit Measure_duration Timespan  "]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.merge(id_tweet, alltweet, on = 'id')\n","df = pd.merge(df, text_tweet, on = 'id')\n","print(df.shape)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mfus8uDTfVDj"},"outputs":[],"source":["a = np.where(df['frame'].isnull())\n","df.drop(a[0], inplace= True)\n","df.reset_index(drop= True, inplace= True)"]},{"cell_type":"markdown","metadata":{"id":"o0hwhb-7fVDk"},"source":["check if there're duplicate tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jyyog01PfVDk"},"outputs":[],"source":["duplicate_t = np.unique(df['id'], return_counts= True)\n","indices = np.where(duplicate_t[1] > 1)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTgGi2oQfVDl"},"outputs":[],"source":["dupl_inx = dict()\n","for i in range(len(duplicate_t[0])):\n","    if duplicate_t[1][i] > 1:\n","        dupl_inx[duplicate_t[0][i]] = duplicate_t[1][i]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0Wb0G2JfVDm"},"outputs":[],"source":["# remove duplicate ids except one, O(459*n) very slow\n","# BETTER NOT TO RUN THIS CODE AGAIN: it is quite slow (about 4 minutes);\n","# however the resulting dataset is saved a couple of cells below and it is then loaded for subsequently analysis as 'text_dataset.csv', so you can see from there\n","\n","for k,v in dupl_inx.items():\n","        for i,row in df.iterrows():\n","            if v > 1:\n","                if row['id'] == k:\n","                    df.drop(i, axis = 0, inplace=True)\n","                    v-=1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lx61TEzDfVDn","outputId":"59ae6625-ce1f-4237-a9d2-54543a11da37"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["max(np.unique(df['id'], return_counts= True)[1])"]},{"cell_type":"markdown","metadata":{"id":"rulILjNxfVDo"},"source":["## Add frames to the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYD2xj9WfVDo"},"outputs":[],"source":["frames = set()\n","i = 0\n","for row in df['frame']:\n","    cat = row.split(' ')\n","    for c in cat:\n","        frames.add(c)\n","    i += 1\n","\n","for f in frames:\n","    df[f] = df['frame'].str.contains(f)\n","    df[f] = df[f].astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8SLNyaXfVDp"},"outputs":[],"source":["df.to_csv('text_dataset.csv', sep = ',', header= True, index=False)"]},{"cell_type":"markdown","metadata":{"id":"NHiHGQQQfVDp"},"source":["# Text cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJQfBunjfVDq"},"outputs":[],"source":["# define my punctuation\n","my_punct = ['\"', '$', '%', '&', \"'\", '(', ')', '*', '+', ',',\n","           '/', ':', ';', '<', '=', '>', '@', '[', '\\\\', ']', '^', '_',\n","           '`', '{', '|', '}', '~', '»', '«', '“', '”', '#', ]\n","\n","punct_pattern = re.compile(\"[\" + re.escape(\"\".join(my_punct)) + \"]\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLLs2navfVDr"},"outputs":[],"source":["def clean_text(text):\n","    #remove tag and links\n","    ctext = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n","    emo = re.findall(r'\\w*[:;=][-D3BPcoO\\)\\(\\\\]\\(*\\)*|<3|XD|\\^\\-\\^', ctext)\n","    hashtag = re.findall(r'#\\w+', text)\n","    #remove double spaces\n","    ctext = re.sub(r\"  \", \" \", ctext)\n","    # instagram/images link\n","    ctext = re.sub(r'instagram.com/\\w*/\\w*[.-]|pinterest.com/pin/\\d+|pic.twitter.com/\\w+|https?://\\S+|www\\.\\S+|\\w+\\.\\w+\\/\\w+', \"\", ctext)\n","    #'useless' punctuation\n","    ctext = re.sub(punct_pattern, \"\", ctext)\n","    #points only after words\n","    ###\n","    #return only the presence/absence of an emoticon/hashtag in the text\n","    return ctext, len(emo)>0, len(hashtag)>0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4pYrilUfVDs"},"outputs":[],"source":["file_tsv = \"dataset_raw/text_dataset.csv\"\n","df = pd.read_csv(file_tsv, encoding='utf-8') #names=['id','text'],\n","\n","# define function to clean the text of a dataframe\n","# for this task we pass all the dataset and clean the data from\n","# the column 'text'\n","def clean(df):\n","    cleaned_text = []\n","    exc_mark = []\n","    emoticon = []\n","    hashtag = []\n","    text_proc = []\n","    # stopword deletion\n","    stop_words = set(stopwords.words('english'))\n","    for tweet in df['text']:\n","        ct, emo, hash = clean_text(tweet.lower())\n","        ct = [word for word in ct.split() if word not in stop_words] #split the text\n","        text_proc.append(' '.join(ct)) # merge the part in the list, so it's one string\n","        cleaned_text.append(ct)\n","        emoticon.append(int(emo))\n","        hashtag.append(int(hash))\n","        #exc_mark.append(count_thing(tweet, '!'))\n","\n","    df['text_list'] = cleaned_text\n","    df['text'] = text_proc\n","    df['emoticon'] = emoticon\n","    df['hashtag'] = hashtag\n","\n","clean(df)"]},{"cell_type":"markdown","metadata":{"id":"1WIp6LSpfVDs"},"source":["# Feature enrichment\n","<br> Here we add features coming from the paper [Francesco Barbieri and Horacio Saggion, 2014, Modelling Irony in Twitter](https://aclanthology.org/E14-3007.pdf), which  are *Syno_Lower_Mean* and *Syn_Mean*. These features quantify the use of synonyms in tweets by calculating the mean frequency of selected synonyms with frequencies lower than the original word (Syno_Lower_Mean) and all synonyms (Syn_Mean), providing insights into the intentional use of less common words in ironic communication\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPYwLSHafVDt"},"outputs":[],"source":["# function to compute the features from text of the tweets\n","# df is passed as parameter to add directly the columns\n","# returns all the data structures containing polarity, sentics values,....\n","def add_feature(df):\n","    #define all the structures for the data\n","    syno_lower_mean = []\n","    syn_mean = []\n","    polarity_label = []\n","    sentics_values = {}\n","    moodtags = []\n","    semantics = []\n","    all_moods = set()\n","\n","    for tweet in df['text_list']:\n","        #inizialize aggragate values for the tweet\n","        sum_syn = 0\n","        n_synset = 0\n","        polarity = []\n","        tweet_values = dict()\n","        mood_data = []\n","        sem_data = []\n","\n","        for w in tweet:\n","\n","            try:\n","                #retrieve all the sentic data for a word\n","                senticnet_data = sn.concept(w)\n","\n","                #sum the values by key, if the word is the first, the dictionary is initialized\n","                diz_sen = senticnet_data['sentics']\n","                for k,v in diz_sen.items():\n","                    tweet_values[k] = tweet_values.get(k, 0) + float(v)\n","\n","                mood_data.extend([tag.lstrip('#') for tag in senticnet_data['moodtags']]) # Remove '#' from moodtags to create an additional label\n","\n","                for t in mood_data:\n","                    all_moods.add(t)\n","\n","                sem_data.extend([tag.lstrip('#') for tag in senticnet_data['semantics']]) #same thing\n","\n","\n","                polarity.append(senticnet_data['polarity_label'])\n","\n","            except:\n","                None\n","\n","            # feature from the paper\n","            threshold = word_frequency(w, 'en') # set the frequency value for the word\n","            n_synset += len(wordnet.synsets(w, lang = 'eng')) #retrieve the synsets number for the tweet\n","\n","            #sum all the rare synonyms of a word\n","            for syn in wordnet.synsets(w, lang='eng'):\n","                for i in syn.lemmas():\n","                    if word_frequency(i.name(), 'en') < threshold:\n","                        sum_syn += 1\n","\n","        # polarity of a tweet\n","        v,c = np.unique(polarity, return_counts=True)\n","        if len(c) > 0 and len(v) > 0 :\n","            polarity_label.append(list(v)[np.argmax(list(c))]) #majority voting for the tweets polarity\n","        else:\n","            polarity_label.append('neutral') #neutral for the ones without information\n","\n","        #mean values\n","        if len(tweet_values) > 0:\n","            for k,v in tweet_values.items():\n","                sentics_values[k] = sentics_values.get(k, []) + [v/len(tweet)]\n","        else:\n","            for k,v in sentics_values.items():\n","                sentics_values[k] = sentics_values.get(k, []) + [0]\n","\n","        syno_lower_mean.append(sum_syn/len(tweet))\n","        syn_mean.append(n_synset/len(tweet))\n","        moodtags.append(list(set(mood_data)))\n","        semantics.append(list(set(sem_data)))\n","\n","    # first version of the dataset\n","    df['syno_lower_mean'] = syno_lower_mean\n","    df['syn_mean'] = syn_mean\n","    return polarity_label, sentics_values, moodtags, all_moods, semantics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xdf07YzfVDv"},"outputs":[],"source":["polarity_label, sentics_values, moodtags, all_moods, semantics = add_feature(df)"]},{"cell_type":"markdown","metadata":{"id":"x3b2Lm-5fVDx"},"source":["## Here we create different versions of the same dataset <br>\n","first we simply balance the classes and use the data as they are, label, text, ids, and frames"]},{"cell_type":"markdown","source":["# Imbalance Learning"],"metadata":{"id":"cO2JTOlWgnXc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F9V46E6SfVDy"},"outputs":[],"source":["X = df.drop('score', axis = 1)\n","X = X.drop('text_list', axis = 1)\n","y = (df['score'] < 0).astype(int)\n","\n","minority = min(y.value_counts())\n","\n","rus = RandomUnderSampler(sampling_strategy={0: minority, 1: minority}, random_state=42)\n","X_resampled, y_resampled = rus.fit_resample(X, y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.33, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"tiKepcTwfVDy"},"source":["# Standardization of the syn_mean and syno_lower_mean columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_hfdZ3SfVDz"},"outputs":[],"source":["scaler = MinMaxScaler()\n","c = X_train['syn_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","X_train['syn_mean'] = scaler.transform(c)\n","X_test['syn_mean'] = scaler.transform(X_test['syn_mean'].values.reshape(-1,1))\n","\n","scaler = MinMaxScaler()\n","c = X_train['syno_lower_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","X_train['syno_lower_mean'] = scaler.transform(c)\n","X_test['syno_lower_mean'] = scaler.transform(X_test['syno_lower_mean'].values.reshape(-1,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otOoOCfQfVD0"},"outputs":[],"source":["y_train = y_train.values.reshape(-1,1)\n","X_train.reset_index(inplace = True, drop = True)\n","\n","y_test = y_test.values.reshape(-1,1)\n","X_test.reset_index(inplace = True, drop = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlh3JvNVfVD0"},"outputs":[],"source":["df_train = pd.concat([pd.DataFrame(y_train), X_train], axis = 1)\n","df_test = pd.concat([pd.DataFrame(y_test), X_test], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMamkUiRfVD1"},"outputs":[],"source":["df_train.to_csv('dataset_first_task/traindata_frames_syno_nofeatureselection.csv', index=False)\n","df_test.to_csv('dataset_first_task/testdata_frames_syno_nofeatureselection.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"MIaagr76fVD2"},"source":["## Then we use a version with the 1000 most frequent words after the tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CC3GK0bIfVD2"},"outputs":[],"source":["vect = CountVectorizer(stop_words='english', max_features = 1000)\n","vect.fit(X_train['text'])\n","train_vect_df = vect.transform(X_train['text'])\n","test_vect_df = vect.transform(X_test['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ev3h_9SlfVD2"},"outputs":[],"source":["train_vect_df = pd.DataFrame(train_vect_df.toarray())\n","train_vect_df.columns = vect.get_feature_names_out()\n","\n","test_vect_df = pd.DataFrame(test_vect_df.toarray())\n","test_vect_df.columns = vect.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWaah3CgfVD3"},"outputs":[],"source":["df_train = pd.concat([df_train, train_vect_df], axis = 1)\n","df_train.to_csv('dataset_first_task/traindata_frames_wordvect_syno_nofeatureselection.csv', index=False)\n","df_test = pd.concat([df_test, test_vect_df], axis = 1)\n","df_test.to_csv('dataset_first_task/testdata_frames_wordvect_syno_nofeatureselection.csv', index = False)"]},{"cell_type":"markdown","metadata":{"id":"9XmTQ4mKfVD4"},"source":["## Then a version adding senticnet features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTyvGptmfVD5"},"outputs":[],"source":["df['polarities'] = polarity_label\n","df['polarities'] = le.fit_transform(df['polarities'])\n","for k,v in sentics_values.items():\n","    df[k] = v\n","df['mood'] = moodtags\n","df['semantics'] = semantics\n","\n","#create moodtags column\n","columns = dict()\n","for i,r in df.iterrows():\n","    for e in all_moods:\n","        if e in r['mood']:\n","            columns[e] = columns.get(e, []) + [1]\n","        else:\n","            columns[e] = columns.get(e, []) + [0]\n","\n","for k,v in columns.items():\n","    df[k+'_mood'] = v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2yOqQJXYfVD6"},"outputs":[],"source":["#create a string with all the semantics for a tweet\n","prova_sem = []\n","for t in df['semantics']:\n","    all = ''\n","    for v in t:\n","        all += v + ' '\n","    prova_sem.append(all)\n","df['semantics'] = prova_sem\n","\n","#use countvectorizer to create all the semantics columns\n","vect = CountVectorizer()\n","vect = CountVectorizer(stop_words='english')\n","vect.fit(df['semantics'])\n","sentic_vect_df = vect.transform(df['semantics'])\n","sentic_vect_df = pd.DataFrame(sentic_vect_df.toarray())\n","sentic_vect_df.columns = vect.get_feature_names_out()\n","df.reset_index(inplace=True, drop = True)\n","df = pd.concat([df, sentic_vect_df], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwEcu-yKfVD7"},"outputs":[],"source":["#drop redundant columns\n","df.drop('mood', axis = 1, inplace = True)\n","df.drop('semantics', axis = 1, inplace = True)\n","df.drop('text_list', axis = 1, inplace= True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gPdVQjq-fVD7"},"outputs":[],"source":["df_2 = df.copy() # copy only for 'safety' pourposes, for the second task i use the original"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahHCo2fOfVD8"},"outputs":[],"source":["X = df_2.drop('score', axis = 1)\n","y = df_2['score']\n","y = (y < 0).astype(int)\n","\n","minority = min(y.value_counts())\n","\n","rus = RandomUnderSampler(sampling_strategy={0: minority, 1: minority}, random_state=42)\n","X_resampled, y_resampled = rus.fit_resample(X, y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.33, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d-UEWRElfVD9"},"outputs":[],"source":["y_train = y_train.values.reshape(-1,1)\n","X_train.reset_index(inplace = True, drop = True)\n","\n","y_test = y_test.values.reshape(-1,1)\n","X_test.reset_index(inplace = True, drop = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcqTx0kNfVD9"},"outputs":[],"source":["train_df = pd.concat([pd.DataFrame(y_train), X_train], axis = 1)\n","test_df = pd.concat([pd.DataFrame(y_test), X_test], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNu7n_P-fVD-"},"outputs":[],"source":["scaler = MinMaxScaler()\n","c = train_df['syno_lower_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","train_df['syno_lower_mean'] = scaler.transform(c)\n","test_df['syno_lower_mean'] = scaler.transform(test_df['syno_lower_mean'].values.reshape(-1,1))\n","\n","scaler = MinMaxScaler()\n","c = train_df['syn_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","train_df['syn_mean'] = scaler.transform(c)\n","test_df['syn_mean'] = scaler.transform(test_df['syn_mean'].values.reshape(-1,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fa80J4BBfVD-"},"outputs":[],"source":["train_df.to_csv('dataset_first_task/traindata_frames_syno_sentic_nofeatureselection.csv', index = False)\n","test_df.to_csv('dataset_first_task/testdata_frames_syno_sentic_nofeatureselection.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gv9vTVvtfVD_"},"outputs":[],"source":["vect = CountVectorizer(stop_words='english', max_features = 1000)\n","vect.fit(X_train['text'])\n","train_vect_df = vect.transform(X_train['text'])\n","test_vect_df = vect.transform(X_test['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fcH_JjDfVD_"},"outputs":[],"source":["train_vect_df = pd.DataFrame(train_vect_df.toarray())\n","train_vect_df.columns = vect.get_feature_names_out()\n","\n","test_vect_df = pd.DataFrame(test_vect_df.toarray())\n","test_vect_df.columns = vect.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-aoHD_11fVEB"},"outputs":[],"source":["train_vect_df.reset_index(inplace=True)\n","test_vect_df.reset_index(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7m0RQyFfVEC"},"outputs":[],"source":["train_df = pd.concat([train_df, train_vect_df], axis = 1)\n","test_df = pd.concat([test_df, test_vect_df], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ANkW0dUfVEC"},"outputs":[],"source":["train_df.to_csv('dataset_first_task/traindata_frames_wordvect_syno_sentic_nofeatureselection.csv', index = False)\n","test_df.to_csv('dataset_first_task/testdata_frames_wordvect_syno_sentic_nofeatureselection.csv', index = False)"]},{"cell_type":"markdown","metadata":{"id":"BjMVBwpvfVED"},"source":["# Feature selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rdu0pzOEfVEE"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectFromModel\n","from imblearn.under_sampling import RandomUnderSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAb-1E5_fVEE"},"outputs":[],"source":["X = train_df.drop(0, axis = 1)\n","X = X.iloc[:,3:] #remove text and\n","y = train_df[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPgZrm88fVEF"},"outputs":[],"source":["np.random.seed(42)\n","\n","feature_names = np.array(X.columns)"]},{"cell_type":"markdown","metadata":{"id":"r2eHafMTfVEF"},"source":["## random decision trees to check how many times a feature is important in the classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPvj6NC0fVEG","outputId":"21674450-d15f-44d5-df72-dfc30d521102"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('literally', 130), ('syno_lower_mean', 128), ('syn_mean', 88), ('attitude', 74), ('speak', 51), ('index', 38), ('introspection', 25), ('disgust_mood', 19), ('sensitivity', 15), ('hashtag', 12), ('temper', 4), ('polarities', 2), ('fear_mood', 1), ('Questioning', 1), ('Change_direction', 1), ('Mental_property', 1), ('Taking', 1), ('Measure_volume', 1), ('anniversary', 1), ('cool', 1), ('die', 1), ('directorate', 1), ('lust', 1), ('face', 1), ('game', 1), ('jumping', 1), ('Aggregate', 1)]\n"]}],"source":["feature_dict = {}\n","\n","for i in range(200):\n","    crit = np.random.choice(['gini', 'entropy', 'log_loss'])\n","    mss = np.random.uniform(1e-2, 1e0)\n","    msl = np.random.uniform(0.001, 0.2)\n","    md=  np.random.randint(2, 200)\n","\n","    DT = DecisionTreeClassifier(criterion = crit, max_depth= md, min_samples_leaf= msl, min_samples_split=mss, random_state= 42)\n","\n","    threshold =  0.0001\n","\n","    sfm = SelectFromModel(DT, threshold = threshold).fit(X, y)\n","\n","    for i in feature_names[np.array(sfm.get_support())]:\n","        feature_dict[i] = feature_dict.get(i, 0) + 1\n","\n","print(sorted(feature_dict.items(), key = lambda x:x[1], reverse= True))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMM_3wR0fVEH"},"outputs":[],"source":["final_df = pd.concat([y, X[list(feature_dict.keys())], train_df['text']], axis = 1)\n","test_df = pd.read_csv('dataset_first_task/testdata_frames_wordvect_syno_sentic_nofeatureselection.csv')\n","final_df.to_csv('dataset_first_task/traindata_featureselection.csv', index=False)\n","final_test = pd.concat([test_df['0'], test_df[list(feature_dict.keys())], test_df['text'] ], axis = 1)\n","final_test.to_csv('dataset_first_task/testdata_featureselection.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"lp0sw4DmfVEH"},"source":["# Data preprocessing for the second task <br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gjsfT4y5fVEI"},"outputs":[],"source":["df = pd.read_csv('dataset_raw/text_dataset.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxIVj8tZfVEI"},"outputs":[],"source":["sarcastic = pd.read_table('dataset_raw/sarcasticFRAMES.tsv', header= None)\n","sarcastic.columns = ['id', 'score', 'frames']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BY7BRMbOfVEJ"},"outputs":[],"source":["#first standardize the id format\n","ids = []\n","for i in sarcastic['id']:\n","    ids.append(re.sub('traintweets.tsv:|trialtweets.tsv:','' ,i))\n","\n","sarcastic['id'] = ids\n","#retrieve indexes for the sarcastic tweets\n","idxs_sar = []\n","for i in sarcastic['id']:\n","    row = np.where(df['id'] == int(i))[0]\n","    if len(row) > 0:\n","        idxs_sar.append(row[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgXNLZ_cfVEJ"},"outputs":[],"source":["sar_df = df.iloc[idxs_sar,:]\n","sar_df['label'] = [0]*sar_df.shape[0] #set label for the sarcastic tweet as 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZBhDdxVfVEK"},"outputs":[],"source":["#same for ironic\n","iro = pd.read_table('dataset_raw/ironicFRAMES.tsv', header= None)\n","iro.columns = ['id', 'score', 'frames']\n","\n","ids = []\n","for i in iro['id']:\n","    ids.append(re.sub('traintweets.tsv:|trialtweets.tsv:','' ,i))\n","iro['id'] = ids\n","\n","idxs_iro = []\n","for i in iro['id']:\n","    row = np.where(df['id'] == int(i))[0]\n","    if len(row) > 0:\n","        idxs_iro.append(row[0])\n","\n","iro_df = df.iloc[idxs_iro,:]\n","iro_df['label'] = [1]*iro_df.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pvRTzLSfVEL","outputId":"f5e9934f-8fe5-402a-93f1-f6769248b296"},"outputs":[{"data":{"text/plain":["(2991, 723)"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["df_second_task = pd.concat([sar_df, iro_df], axis = 0)\n","df_second_task.drop(['id', 'score'], axis = 1, inplace = True)\n","df_second_task.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQ_Ouq5MfVEL"},"outputs":[],"source":["df_second_task = df_second_task.sample(frac = 1, replace=False, random_state=42)\n","df_second_task.columns = ['text'] + list(df_second_task.columns[1:])"]},{"cell_type":"markdown","metadata":{"id":"XG6Ur403fVEM"},"source":["## Same process for feature engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RglcRKXUfVEN"},"outputs":[],"source":["clean(df_second_task)\n","polarity_label, sentics_values, moodtags, all_moods, semantics = add_feature(df_second_task)"]},{"cell_type":"markdown","metadata":{"id":"4N57ab0XfVEN"},"source":["### Frames and features from the paper"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ea3-9HVAfVEO"},"outputs":[],"source":["df_second_task.columns = ['tweet'] + list(df_second_task.columns[1:])\n","X = df_second_task.drop('label', axis = 1)\n","X = X.drop(['text_list', 'frame'], axis = 1)\n","y = df_second_task['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZ76qWbZfVEO"},"outputs":[],"source":["train_df = pd.concat([y_train, X_train], axis = 1)\n","test_df = pd.concat([y_test, X_test], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzVkTWNRfVEP"},"outputs":[],"source":["scaler = MinMaxScaler()\n","c = train_df['syno_lower_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","train_df['syno_lower_mean'] = scaler.transform(c)\n","test_df['syno_lower_mean'] = scaler.transform(test_df['syno_lower_mean'].values.reshape(-1,1))\n","\n","scaler = MinMaxScaler()\n","c = train_df['syn_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","train_df['syn_mean'] = scaler.transform(c)\n","test_df['syn_mean'] = scaler.transform(test_df['syn_mean'].values.reshape(-1,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jYyXLQlfVEQ"},"outputs":[],"source":["train_df.to_csv('dataset_second_task/ST_train_text_frames_syno.csv', index = False)\n","test_df.to_csv('dataset_second_task/ST_test_text_frames_syno.csv', index = False)"]},{"cell_type":"markdown","metadata":{"id":"S06ZRn5cfVER"},"source":["### With countvectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlfQvR-FfVES"},"outputs":[],"source":["X = df_second_task.drop('label', axis = 1)\n","X = X.drop(['text_list', 'frame'], axis = 1)\n","y = df_second_task['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgIfoe2nfVET"},"outputs":[],"source":["train_df = pd.concat([y_train, X_train], axis = 1)\n","test_df = pd.concat([y_test, X_test], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7z0AxYNjfVEU"},"outputs":[],"source":["scaler = MinMaxScaler()\n","c = train_df['syno_lower_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","train_df['syno_lower_mean'] = scaler.transform(c)\n","test_df['syno_lower_mean'] = scaler.transform(test_df['syno_lower_mean'].values.reshape(-1,1))\n","\n","scaler = MinMaxScaler()\n","c = train_df['syn_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","train_df['syn_mean'] = scaler.transform(c)\n","test_df['syn_mean'] = scaler.transform(test_df['syn_mean'].values.reshape(-1,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJHobAXafVEV"},"outputs":[],"source":["train_df.reset_index(inplace=True, drop = True)\n","test_df.reset_index(inplace=True, drop = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrSexko-fVEV"},"outputs":[],"source":["vect = CountVectorizer(stop_words='english', max_features = 1000)\n","vect.fit(train_df['tweet'])\n","train_vect_df = vect.transform(train_df['tweet'])\n","test_vect_df = vect.transform(test_df['tweet'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ck9q478fVEW"},"outputs":[],"source":["train_vect_df = pd.DataFrame(train_vect_df.toarray())\n","train_vect_df.columns = vect.get_feature_names_out()\n","\n","test_vect_df = pd.DataFrame(test_vect_df.toarray())\n","test_vect_df.columns = vect.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlInliNrfVFQ"},"outputs":[],"source":["train_vect_df.reset_index(inplace=True)\n","test_vect_df.reset_index(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2mloCQpfVFR"},"outputs":[],"source":["train_df = pd.concat([train_df, train_vect_df], axis = 1)\n","test_df = pd.concat([test_df, test_vect_df], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGE-SFmbfVFR"},"outputs":[],"source":["train_df.to_csv('dataset_second_task/ST_train_text_frames_wordvect_syno.csv', index = False)\n","test_df.to_csv('dataset_second_task/ST_test_text_frames_wordvect_syno.csv', index = False)"]},{"cell_type":"markdown","metadata":{"id":"ToxNLz6XfVFS"},"source":["### Sentic features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLnniyNFfVFS"},"outputs":[],"source":["df_second_task['polarities'] = polarity_label\n","df_second_task['polarities'] = le.fit_transform(df_second_task['polarities'])\n","for k,v in sentics_values.items():\n","    df_second_task[k] = v\n","df_second_task['mood'] = moodtags\n","df_second_task['semantics'] = semantics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoOxpTkTfVFT"},"outputs":[],"source":["#create moodtags column\n","columns = dict()\n","for i,r in df_second_task.iterrows():\n","    for e in all_moods:\n","        if e in r['mood']:\n","            columns[e] = columns.get(e, []) + [1]\n","        else:\n","            columns[e] = columns.get(e, []) + [0]\n","\n","for k,v in columns.items():\n","    df_second_task[k+'_mood'] = v"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCgtfizvfVFT"},"outputs":[],"source":["#use countvectorizer to create all the semantics columns\n","prova_sem = []\n","for t in df_second_task['semantics']:\n","    all = ''\n","    for v in t:\n","        all += v + ' '\n","    prova_sem.append(all)\n","df_second_task['semantics'] = prova_sem\n","\n","vect = CountVectorizer()\n","vect = CountVectorizer(stop_words='english')\n","vect.fit(df_second_task['semantics'])\n","sentic_vect_df = vect.transform(df_second_task['semantics'])\n","\n","sentic_vect_df = pd.DataFrame(sentic_vect_df.toarray())\n","sentic_vect_df.columns = vect.get_feature_names_out()\n","\n","df_second_task.reset_index(inplace=True, drop = True)\n","df_second_task = pd.concat([df_second_task, sentic_vect_df], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6K7n-sbfVFV"},"outputs":[],"source":["df_second_task.drop('mood', axis = 1, inplace = True)\n","df_second_task.drop('semantics', axis = 1, inplace = True)\n","df_second_task.drop('text_list', axis = 1, inplace= True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcNvnMhJfVFV"},"outputs":[],"source":["X = df_second_task.drop('label', axis = 1)\n","X = X.drop('frame', axis = 1)\n","y = df_second_task['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","train_df = pd.concat([y_train, X_train], axis = 1)\n","test_df = pd.concat([y_test, X_test], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0PO_WDqfVFW"},"outputs":[],"source":["scaler = MinMaxScaler()\n","c = train_df['syno_lower_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","train_df['syno_lower_mean'] = scaler.transform(c)\n","test_df['syno_lower_mean'] = scaler.transform(test_df['syno_lower_mean'].values.reshape(-1,1))\n","\n","scaler = MinMaxScaler()\n","c = train_df['syn_mean'].values.reshape(-1,1)\n","scaler.fit(c)\n","train_df['syn_mean'] = scaler.transform(c)\n","test_df['syn_mean'] = scaler.transform(test_df['syn_mean'].values.reshape(-1,1))\n","train_df.reset_index(inplace=True, drop = True)\n","test_df.reset_index(inplace=True, drop = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIR1IXZVfVFX"},"outputs":[],"source":["vect = CountVectorizer(stop_words='english', max_features = 1000)\n","vect.fit(train_df['tweet'])\n","train_vect_df = vect.transform(train_df['tweet'])\n","test_vect_df = vect.transform(test_df['tweet'])\n","train_vect_df = pd.DataFrame(train_vect_df.toarray())\n","train_vect_df.columns = vect.get_feature_names_out()\n","\n","test_vect_df = pd.DataFrame(test_vect_df.toarray())\n","test_vect_df.columns = vect.get_feature_names_out()\n","train_vect_df.reset_index(inplace=True)\n","test_vect_df.reset_index(inplace = True)\n","train_df = pd.concat([train_df, train_vect_df], axis = 1)\n","test_df = pd.concat([test_df, test_vect_df], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtmhBQiufVFX"},"outputs":[],"source":["train_df.to_csv('dataset_second_task/ST_train_text_frames_sentic_wordvect_syno.csv', index = False)\n","test_df.to_csv('dataset_second_task/ST_test_text_frames_sentic_wordvect_syno.csv', index = False)"]},{"cell_type":"markdown","metadata":{"id":"Z-oqGH3xfVFY"},"source":["### Feature selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrcYH7_dfVFY"},"outputs":[],"source":["X = train_df.iloc[:,2:] #remove text and label\n","y = train_df['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jhA4b-HfVFY","outputId":"039f6f20-60ca-4fab-b10e-c615c7882472"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('introspection', 200), ('Calendric_unit', 91), ('syn_mean', 84), ('index', 74), ('fear_mood', 26), ('temper', 23), ('proactive', 19), ('syno_lower_mean', 14), ('Awareness', 7), ('enjoy', 7), ('really', 7), ('eagerness_mood', 7), ('sensitivity', 7), ('hashtag', 4), ('love', 3), ('attitude', 2), ('eager', 2), ('im', 2), ('sadness_mood', 1), ('Change_direction', 1), ('Perception_active', 1), ('Perception_experience', 1), ('Cause_impact', 1), ('Familiarity', 1), ('Aesthetics', 1), ('Stimulus_focus', 1), ('Obviousness', 1), ('Leadership', 1), ('cloud_nine', 1), ('smile', 1), ('thanks', 1), ('thoroughly_enjoy', 1), ('called', 1), ('guy', 1), ('irony', 1), ('People', 1), ('sexuality', 1)]\n"]}],"source":["np.random.seed(42)\n","\n","DT = DecisionTreeClassifier(random_state= 42)\n","feature_names = np.array(X.columns)\n","feature_dict = {}\n","\n","for i in range(200):\n","    crit = np.random.choice(['gini', 'entropy', 'log_loss'])\n","    mss = np.random.uniform(1e-2, 1e0)\n","    msl = np.random.uniform(0.001, 0.2)\n","    md=  np.random.randint(2, 200)\n","\n","    DT = DecisionTreeClassifier(criterion = crit, max_depth= md, min_samples_leaf= msl, min_samples_split=mss, random_state= 42)\n","\n","    threshold =  0.0001\n","\n","    sfm = SelectFromModel(DT, threshold = threshold).fit(X, y)\n","\n","    for i in feature_names[np.array(sfm.get_support())]:\n","        feature_dict[i] = feature_dict.get(i, 0) + 1\n","\n","print(sorted(feature_dict.items(), key = lambda x:x[1], reverse= True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-T_wHZyfVFa"},"outputs":[],"source":["train_df = pd.read_csv('dataset_second_task/ST_train_text_frames_sentic_wordvect_syno.csv')\n","final_df = pd.concat([train_df['label'], train_df[list(feature_dict.keys())], train_df['tweet']], axis = 1)\n","test_df = pd.read_csv('dataset_second_task/ST_test_text_frames_sentic_wordvect_syno.csv')\n","final_test = pd.concat([test_df['label'], test_df[list(feature_dict.keys())], test_df['tweet'] ], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UO76VXsTfVFa"},"outputs":[],"source":["final_df.to_csv('dataset_second_task/ST_traindata_featureselection.csv', index=False)\n","final_test.to_csv('dataset_second_task/ST_testdata_featureselection.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}